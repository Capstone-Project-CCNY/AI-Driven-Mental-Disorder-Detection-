{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "064b4112a1bd4f6b91c7658ff55853f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b54fc197b82840199451d079c2c5dfdd",
              "IPY_MODEL_69a3a57061014106ae7cee507411b2c0",
              "IPY_MODEL_27204e73a55f48e19cd2b46c0c3f5402"
            ],
            "layout": "IPY_MODEL_0e1c10f139bc45efab698ae118077e2c"
          }
        },
        "b54fc197b82840199451d079c2c5dfdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81a86abf6d844a49ba70a4be981a82d1",
            "placeholder": "​",
            "style": "IPY_MODEL_02fe6bba85964c27b203c7ec990fc812",
            "value": "model.safetensors: 100%"
          }
        },
        "69a3a57061014106ae7cee507411b2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cda232055cce421183ad653a95ff6fa4",
            "max": 791103952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce3f31fdc61b47149f63bbc7927701df",
            "value": 791103952
          }
        },
        "27204e73a55f48e19cd2b46c0c3f5402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b62470434ac04460a0c6d4f602d822b2",
            "placeholder": "​",
            "style": "IPY_MODEL_bd20ce70c0b34d31a10676ef1691a209",
            "value": " 791M/791M [00:06&lt;00:00, 170MB/s]"
          }
        },
        "0e1c10f139bc45efab698ae118077e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a86abf6d844a49ba70a4be981a82d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02fe6bba85964c27b203c7ec990fc812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cda232055cce421183ad653a95ff6fa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce3f31fdc61b47149f63bbc7927701df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b62470434ac04460a0c6d4f602d822b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd20ce70c0b34d31a10676ef1691a209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "0_K6ooVbcHB3",
        "outputId": "7c03a259-d436-4e37-d1ef-da512fca9c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a625844-e06f-4aab-a39a-bd548228cfb1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a625844-e06f-4aab-a39a-bd548228cfb1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"sibbir\",\"key\":\"c9b1f65daec00b23325e5e866738f3b6\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLGmv3FOMbG5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Create a Kaggle directory\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Move kaggle.json to the correct directory\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "\n",
        "# Set correct permissions\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"shuvoalok/raf-db-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "OMwe86ZdMmkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7436a9c-09fe-4026-dab9-e587fa9fde3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shuvoalok/raf-db-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37.7M/37.7M [00:01<00:00, 26.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import timm\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8e5zk-JSceIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_csv_path = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/train_labels.csv\"\n",
        "test_csv_path = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/test_labels.csv\"\n",
        "\n",
        "# Load CSV files\n",
        "train_labels = pd.read_csv(train_csv_path)\n",
        "test_labels = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Show first few rows\n",
        "print(\"Train CSV Sample:\")\n",
        "print(train_labels.head())\n",
        "\n",
        "print(\"\\nTest CSV Sample:\")\n",
        "print(test_labels.head())"
      ],
      "metadata": {
        "id": "l6JJswDAMrXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d21eef-b424-4602-9f50-f5517392e2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train CSV Sample:\n",
            "                     image  label\n",
            "0  train_00001_aligned.jpg      5\n",
            "1  train_00002_aligned.jpg      5\n",
            "2  train_00003_aligned.jpg      4\n",
            "3  train_00004_aligned.jpg      4\n",
            "4  train_00005_aligned.jpg      5\n",
            "\n",
            "Test CSV Sample:\n",
            "                   image  label\n",
            "0  test_0001_aligned.jpg      5\n",
            "1  test_0002_aligned.jpg      1\n",
            "2  test_0003_aligned.jpg      4\n",
            "3  test_0004_aligned.jpg      1\n",
            "4  test_0005_aligned.jpg      5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all unique labels from the train and test datasets\n",
        "print(\"Unique labels in train dataset:\", train_labels[\"label\"].unique())\n",
        "print(\"Unique labels in test dataset:\", test_labels[\"label\"].unique())"
      ],
      "metadata": {
        "id": "2jb9Lg9hMtwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1f21c9-26c4-46e0-9cfe-9221a06436ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in train dataset: [5 4 1 6 2 3 7]\n",
            "Unique labels in test dataset: [5 1 4 3 6 2 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/DATASET\"\n",
        "\n",
        "# List a few sample files from the dataset directory\n",
        "sample_files = os.listdir(dataset_path)[:10]\n",
        "print(\"Sample files in dataset directory:\", sample_files)"
      ],
      "metadata": {
        "id": "bwTJHQIAMvrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc3fbeb-5205-4865-c624-67f82aaf26de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample files in dataset directory: ['train', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_path = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/DATASET/train\"\n",
        "test_path = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/DATASET/test\"\n",
        "\n",
        "# Check inside one of the numbered folders\n",
        "for label in os.listdir(train_path):\n",
        "    label_folder = os.path.join(train_path, label)\n",
        "    if os.path.isdir(label_folder):  # Ensure it's a folder\n",
        "        print(f\"Sample images in {label}:\", os.listdir(label_folder)[:10])"
      ],
      "metadata": {
        "id": "zRcmpIPxMyPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53388ae-09da-4cce-df2a-f531cd94fe84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample images in 3: ['train_09634_aligned.jpg', 'train_03605_aligned.jpg', 'train_09738_aligned.jpg', 'train_09675_aligned.jpg', 'train_09499_aligned.jpg', 'train_09646_aligned.jpg', 'train_03989_aligned.jpg', 'train_09409_aligned.jpg', 'train_04311_aligned.jpg', 'train_01874_aligned.jpg']\n",
            "Sample images in 4: ['train_03528_aligned.jpg', 'train_04784_aligned.jpg', 'train_03592_aligned.jpg', 'train_02812_aligned.jpg', 'train_01942_aligned.jpg', 'train_06397_aligned.jpg', 'train_09067_aligned.jpg', 'train_08352_aligned.jpg', 'train_02918_aligned.jpg', 'train_00310_aligned.jpg']\n",
            "Sample images in 5: ['train_00835_aligned.jpg', 'train_06008_aligned.jpg', 'train_01180_aligned.jpg', 'train_01088_aligned.jpg', 'train_07217_aligned.jpg', 'train_04418_aligned.jpg', 'train_04197_aligned.jpg', 'train_07984_aligned.jpg', 'train_07829_aligned.jpg', 'train_01366_aligned.jpg']\n",
            "Sample images in 7: ['train_09898_aligned.jpg', 'train_09826_aligned.jpg', 'train_10034_aligned.jpg', 'train_10329_aligned.jpg', 'train_10568_aligned.jpg', 'train_12209_aligned.jpg', 'train_09948_aligned.jpg', 'train_11819_aligned.jpg', 'train_10497_aligned.jpg', 'train_09779_aligned.jpg']\n",
            "Sample images in 6: ['train_08087_aligned.jpg', 'train_08803_aligned.jpg', 'train_08183_aligned.jpg', 'train_08168_aligned.jpg', 'train_04993_aligned.jpg', 'train_01235_aligned.jpg', 'train_03874_aligned.jpg', 'train_09085_aligned.jpg', 'train_08911_aligned.jpg', 'train_06414_aligned.jpg']\n",
            "Sample images in 2: ['train_09339_aligned.jpg', 'train_09271_aligned.jpg', 'train_09277_aligned.jpg', 'train_09350_aligned.jpg', 'train_09340_aligned.jpg', 'train_09193_aligned.jpg', 'train_09371_aligned.jpg', 'train_07302_aligned.jpg', 'train_09249_aligned.jpg', 'train_09338_aligned.jpg']\n",
            "Sample images in 1: ['train_00675_aligned.jpg', 'train_04527_aligned.jpg', 'train_06772_aligned.jpg', 'train_01268_aligned.jpg', 'train_03984_aligned.jpg', 'train_04055_aligned.jpg', 'train_08002_aligned.jpg', 'train_07111_aligned.jpg', 'train_03163_aligned.jpg', 'train_07388_aligned.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Define correct paths\n",
        "dataset_path_train = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/DATASET/train\"\n",
        "dataset_path_test = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/DATASET/test\"\n",
        "\n",
        "train_csv_path = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/train_labels.csv\"\n",
        "test_csv_path = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/test_labels.csv\"\n",
        "\n",
        "train_dir = \"/root/rafdb/train\"\n",
        "test_dir = \"/root/rafdb/test\"\n",
        "\n",
        "# Load CSV labels\n",
        "train_labels = pd.read_csv(train_csv_path)\n",
        "test_labels = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Define label mapping\n",
        "label_map = {\n",
        "    1: \"Happy\",\n",
        "    2: \"Sad\",\n",
        "    3: \"Surprise\",\n",
        "    4: \"Fear\",\n",
        "    5: \"Disgust\",\n",
        "    6: \"Anger\",\n",
        "    7: \"Contempt\"\n",
        "}\n",
        "\n",
        "# Create class directories\n",
        "for label in label_map.values():\n",
        "    os.makedirs(os.path.join(train_dir, label), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, label), exist_ok=True)\n",
        "\n",
        "print(\"Class folders created successfully.\")\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def move_images(labels_df, source_base_dir, target_base_dir, label_map):\n",
        "    \"\"\"Move images based on labels dataframe to organized class directories.\"\"\"\n",
        "    for _, row in labels_df.iterrows():\n",
        "        image_name = row[\"image\"]\n",
        "        label_id = row[\"label\"]\n",
        "\n",
        "        if label_id not in label_map:\n",
        "            continue\n",
        "\n",
        "        class_name = label_map[label_id]\n",
        "        src = Path(source_base_dir) / str(label_id) / image_name\n",
        "        dest = Path(target_base_dir) / class_name / image_name\n",
        "\n",
        "        # Create target directory if it doesn't exist\n",
        "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dest))\n",
        "        else:\n",
        "            print(f\"Warning: {src} not found!\")\n",
        "\n",
        "# Move train images\n",
        "move_images(train_labels, dataset_path_train, train_dir, label_map)\n",
        "\n",
        "# Move test images\n",
        "move_images(test_labels, dataset_path_test, test_dir, label_map)\n",
        "\n",
        "print(\"Dataset successfully organized.\")"
      ],
      "metadata": {
        "id": "hOZIH3HTM3DX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f042412b-481d-420d-fdd0-2e7507c3c280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class folders created successfully.\n",
            "Dataset successfully organized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing and augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "SovCUKs-dPfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "class CustomViT(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(CustomViT, self).__init__()\n",
        "        self.model = timm.create_model(\"vit_large_patch16_224\", pretrained=True, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "aSsfRc_VdjTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "train_dir = \"/root/rafdb/train\"\n",
        "test_dir = \"/root/rafdb/test\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "# Use batch size 8 to prevent crashes\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "# Print final class labels\n",
        "class_names = train_dataset.classes\n",
        "print(f\"Classes used for training: {class_names}\")"
      ],
      "metadata": {
        "id": "X9yNZb_KO5P2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb757f0e-fe8e-4ff7-c38e-41ec7f6181f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes used for training: ['Anger', 'Contempt', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ConvNeXt Large\n",
        "model = timm.create_model(\"convnext_large\", pretrained=True, num_classes=len(class_names))\n",
        "model.to(device)\n",
        "\n",
        "# Enable mixed precision\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"ConvNeXt model loaded with mixed precision enabled.\")"
      ],
      "metadata": {
        "id": "6jHSKiqyO_KG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "064b4112a1bd4f6b91c7658ff55853f6",
            "b54fc197b82840199451d079c2c5dfdd",
            "69a3a57061014106ae7cee507411b2c0",
            "27204e73a55f48e19cd2b46c0c3f5402",
            "0e1c10f139bc45efab698ae118077e2c",
            "81a86abf6d844a49ba70a4be981a82d1",
            "02fe6bba85964c27b203c7ec990fc812",
            "cda232055cce421183ad653a95ff6fa4",
            "ce3f31fdc61b47149f63bbc7927701df",
            "b62470434ac04460a0c6d4f602d822b2",
            "bd20ce70c0b34d31a10676ef1691a209"
          ]
        },
        "outputId": "2d544623-4d74-413b-dbf7-318f500764fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/791M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "064b4112a1bd4f6b91c7658ff55853f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNeXt model loaded with mixed precision enabled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-81b482fe9985>:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "metadata": {
        "id": "YAvdQGoIPBz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c584000e-f4f3-4748-9948-4a3eaa688840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Dict\n",
        "import datetime\n",
        "# Importing necessary modules\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Configuration Constants\n",
        "CONFIG = {\n",
        "    \"checkpoint_dir\": \"/content/checkpoints/\",  # Directory to save checkpoints\n",
        "    \"num_classes\": 7,\n",
        "    \"model_name\": \"convnext_large\",\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"num_epochs\": 20,\n",
        "    \"accumulation_steps\": 2,\n",
        "    \"save_interval\": 5,  # Save every 5 epochs\n",
        "    \"mixed_precision\": True  # Use AMP for faster training\n",
        "}\n",
        "\n",
        "# Ensure checkpoint directory exists\n",
        "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
        "\n",
        "# Define Model\n",
        "class CustomConvNeXt(nn.Module):\n",
        "    def __init__(self, num_classes: int = CONFIG[\"num_classes\"]):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            CONFIG[\"model_name\"],\n",
        "            pretrained=False,  # No pretrained weights\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Get the best available device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "def initialize_model(device: torch.device) -> nn.Module:\n",
        "    \"\"\"Initialize model with proper weight initialization.\"\"\"\n",
        "    model = CustomConvNeXt().to(device)\n",
        "\n",
        "    # Initialize weights properly\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    model.apply(_init_weights)\n",
        "    return model\n",
        "\n",
        "def save_checkpoint(\n",
        "    model: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    epoch: int,\n",
        "    loss: float,\n",
        "    path: str\n",
        ") -> None:\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)\n",
        "    print(f\"Checkpoint saved to {path}\")\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    accumulation_steps: int,\n",
        "    epoch: int\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Train model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=False) as pbar:\n",
        "        for batch_idx, (images, labels) in enumerate(pbar):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=CONFIG[\"mixed_precision\"]):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels) / accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item() * accumulation_steps\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': running_loss / (batch_idx + 1),\n",
        "                'acc': 100. * correct / total\n",
        "            })\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100.0 * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def validate(\n",
        "    model: nn.Module,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Validate model performance.\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(), tqdm(test_loader, desc=\"Validating\", leave=False) as pbar:\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'val_loss': val_loss / (pbar.n + 1),\n",
        "                'val_acc': 100. * correct / total\n",
        "            })\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_acc = 100.0 * correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    num_epochs: int = CONFIG[\"num_epochs\"]\n",
        ") -> Dict[str, list]:\n",
        "    \"\"\"Main training loop.\"\"\"\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    start_time = datetime.datetime.now()\n",
        "    print(f\"Training started at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Training on {device} with config: {CONFIG}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train phase\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device,\n",
        "            CONFIG[\"accumulation_steps\"], epoch\n",
        "        )\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % CONFIG[\"save_interval\"] == 0 or (epoch + 1) == num_epochs:\n",
        "            checkpoint_path = os.path.join(\n",
        "                CONFIG[\"checkpoint_dir\"],\n",
        "                f\"checkpoint_epoch_{epoch + 1}.pt\"\n",
        "            )\n",
        "            save_checkpoint(model, optimizer, epoch + 1, val_loss, checkpoint_path)\n",
        "\n",
        "    training_time = datetime.datetime.now() - start_time\n",
        "    print(f\"Training completed in {training_time}\")\n",
        "    return history\n",
        "\n",
        "def main():\n",
        "    # Initialize\n",
        "    device = get_device()\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = initialize_model(device)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CONFIG[\"learning_rate\"],\n",
        "        weight_decay=CONFIG[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load dataset # Moving the dataset loading logic inside main()\n",
        "    train_dir = \"/root/rafdb/train\"\n",
        "    test_dir = \"/root/rafdb/test\"\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "    test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "    # Use batch size 8 to prevent crashes\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Train the model\n",
        "    history = train_model(\n",
        "        model, train_loader, test_loader, criterion, optimizer, device\n",
        "    )\n",
        "\n",
        "    # Save final model\n",
        "    final_model_path = os.path.join(CONFIG[\"checkpoint_dir\"], \"final_model.pt\")\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"Final model saved to {final_model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XJufEWxwPITF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "0d87a380-91f1-4a92-92de-179201397192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/rafdb/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b4fb538889d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-b4fb538889d8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/root/rafdb/test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/rafdb/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training and Validation Accuracy Data\n",
        "epochs = list(range(6, 21))\n",
        "train_acc = [97.28, 98.07, 98.18, 98.70, 98.61, 98.81, 99.07, 98.78, 98.92, 99.10, 98.97, 98.97, 99.16, 99.32, 99.10]\n",
        "val_acc = [85.72, 86.83, 86.18, 86.15, 85.72, 85.23, 86.90, 83.87, 84.62, 85.79, 85.56, 83.74, 86.67, 86.38, 85.82]\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_acc, label=\"Train Accuracy\", marker='o', linestyle='-')\n",
        "plt.plot(epochs, val_acc, label=\"Validation Accuracy\", marker='s', linestyle='--')\n",
        "\n",
        "# Labels and Title\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Training vs Validation Accuracy Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1pYy6ulZPNWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from PIL import Image\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # For progress bars\n",
        "from typing import Dict, List, Tuple  # For type hints\n",
        "\n",
        "# Constants (configurable parameters)\n",
        "MODEL_PATH = \"/content/model_epoch_20.pt\"\n",
        "TEST_LABELS_PATH = \"/root/.cache/kagglehub/datasets/shuvoalok/raf-db-dataset/versions/2/test_labels.csv\"\n",
        "TEST_FOLDER = \"/root/rafdb/test/\"\n",
        "BATCH_SIZE = 32  # Increased batch size for better GPU utilization\n",
        "IMAGE_SIZE = 224\n",
        "NUM_CLASSES = 7\n",
        "NUM_WORKERS = min(4, os.cpu_count())  # Optimal number of workers for data loading\n",
        "\n",
        "# Set device with fallback to MPS for Apple Silicon if available\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define label mapping as constant to avoid recreation\n",
        "LABEL_MAP = {\n",
        "    1: \"Happy\",\n",
        "    2: \"Sad\",\n",
        "    3: \"Surprise\",\n",
        "    4: \"Fear\",\n",
        "    5: \"Disgust\",\n",
        "    6: \"Anger\",\n",
        "    7: \"Contempt\"\n",
        "}\n",
        "REVERSE_LABEL_MAP = {v: k-1 for k, v in LABEL_MAP.items()}  # For faster label to index conversion\n",
        "\n",
        "class CustomConvNeXt(nn.Module):\n",
        "    \"\"\"Custom ConvNeXt model with pretrained weights.\"\"\"\n",
        "    def __init__(self, num_classes: int = NUM_CLASSES):\n",
        "        super().__init__()  # More modern syntax\n",
        "        self.model = timm.create_model(\n",
        "            \"convnext_large\",\n",
        "            pretrained=False,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "def load_model(model_path: str) -> nn.Module:\n",
        "    \"\"\"Load and prepare the model for evaluation.\"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "    model = CustomConvNeXt(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    # Load with error handling\n",
        "    try:\n",
        "        model = torch.jit.load(model_path, map_location=device)\n",
        "        model.eval()\n",
        "        print(f\"Model successfully loaded from: {model_path}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading model: {e}\")\n",
        "\n",
        "# Image transformations with optimized parameters\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_ground_truth_labels(csv_path: str) -> Dict[str, str]:\n",
        "    \"\"\"Load and process ground truth labels from CSV.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        return {row[\"image\"]: LABEL_MAP[row[\"label\"]] for _, row in df.iterrows()}\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading ground truth labels: {e}\")\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Optimized dataset class for test images.\"\"\"\n",
        "    def __init__(self, test_folder: str, transform=None):\n",
        "        self.image_paths = glob.glob(os.path.join(test_folder, \"**\", \"*.jpg\"), recursive=True)\n",
        "        if not self.image_paths:\n",
        "            raise FileNotFoundError(f\"No images found in: {test_folder}\")\n",
        "        self.transform = transform\n",
        "        # Pre-load image names for faster lookup\n",
        "        self.image_names = [os.path.basename(p) for p in self.image_paths]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str, str]:\n",
        "        img_path = self.image_paths[idx]\n",
        "        img_name = self.image_names[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, img_path, img_name\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a dummy image if loading fails\n",
        "            dummy_img = torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "            return dummy_img, img_path, img_name\n",
        "\n",
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    test_loader: DataLoader,\n",
        "    ground_truth: Dict[str, str]\n",
        ") -> Tuple[float, Dict[str, float], List[Tuple[str, str, str]]]:\n",
        "    \"\"\"Evaluate model performance and return metrics and misclassified examples.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    misclassified = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=device.type == \"cuda\"):\n",
        "        for images, img_paths, img_names in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            preds = preds.cpu().numpy()\n",
        "\n",
        "            for img_name, img_path, pred in zip(img_names, img_paths, preds):\n",
        "                if img_name not in ground_truth:\n",
        "                    continue\n",
        "\n",
        "                true_label = ground_truth[img_name]\n",
        "                predicted_label = LABEL_MAP[pred + 1]\n",
        "\n",
        "                y_true.append(REVERSE_LABEL_MAP[true_label])\n",
        "                y_pred.append(pred)\n",
        "                total += 1\n",
        "\n",
        "                if true_label == predicted_label:\n",
        "                    correct += 1\n",
        "                else:\n",
        "                    if len(misclassified) < 5:  # Store only top 5 misclassified\n",
        "                        misclassified.append((img_path, true_label, predicted_label))\n",
        "\n",
        "    accuracy = correct / total * 100 if total > 0 else 0\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "    }\n",
        "\n",
        "    return metrics, misclassified\n",
        "\n",
        "def display_predictions(image_paths: List[str], true_labels: List[str], pred_labels: List[str]):\n",
        "    \"\"\"Display images with their true and predicted labels.\"\"\"\n",
        "    num_images = len(image_paths)\n",
        "    if num_images == 0:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, min(5, num_images), figsize=(15, 5))\n",
        "    if num_images == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (img_path, true, pred) in enumerate(zip(image_paths, true_labels, pred_labels)):\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].axis(\"off\")\n",
        "            axes[i].set_title(f\"True: {true}\\nPred: {pred}\", fontsize=10)\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying image {img_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Load model\n",
        "    model = load_model(MODEL_PATH)\n",
        "\n",
        "    # Load ground truth\n",
        "    ground_truth = load_ground_truth_labels(TEST_LABELS_PATH)\n",
        "    print(f\"Loaded {len(ground_truth)} ground truth labels.\")\n",
        "\n",
        "    # Create test dataset and loader\n",
        "    test_dataset = TestDataset(TEST_FOLDER, transform=transform)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True  # Faster data transfer to GPU\n",
        "    )\n",
        "    print(f\"Total test images found: {len(test_dataset)}\")\n",
        "\n",
        "    # Evaluate\n",
        "    metrics, misclassified = evaluate_model(model, test_loader, ground_truth)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nModel Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "    # Display misclassified images\n",
        "    if misclassified:\n",
        "        img_paths, true_labels, pred_labels = zip(*misclassified)\n",
        "        display_predictions(img_paths, true_labels, pred_labels)\n",
        "    else:\n",
        "        print(\"No misclassified images found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "wSYT_5AnPstb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}