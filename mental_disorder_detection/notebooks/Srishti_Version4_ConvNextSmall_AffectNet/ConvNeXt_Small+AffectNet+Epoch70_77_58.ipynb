{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zoM6-2bR2Mt",
    "outputId": "a4594f36-08ea-408b-a774-24b1c1e286e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.25.6)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.1.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
      "Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.6.17\n",
      "    Uninstalling kaggle-1.6.17:\n",
      "      Successfully uninstalled kaggle-1.6.17\n",
      "Successfully installed kaggle-1.7.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle --upgrade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-rWdBdvXOg3r"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PKkS4riTW3W",
    "outputId": "32f06580-b4e4-45ae-9a0e-3e574b03b19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/thienkhonghoc/affectnet\n",
      "License(s): unknown\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d thienkhonghoc/affectnet -p /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QqGmBUfoOlSx"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/affectnet.zip -d /content/affectnet > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ko0mXTVrzPiZ",
    "outputId": "a26553d3-2d2d-497b-e4ca-137dd1d26ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.28.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision timm matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpMSsnaFQ-Dd"
   },
   "outputs": [],
   "source": [
    "#Lower Learning Rate (1e-6)\n",
    "#Better Regularization (dropout=0.5, label_smoothing=0.3)\n",
    "#More Controlled Data Augmentation\n",
    "#Stochastic Weight Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVKyAMHxkHXB",
    "outputId": "c4bb51ef-1aab-423d-a78a-573d3cb5485c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\n",
      "Fine-tuning with Stronger Regularization and Stability...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-cadea068683f>:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "<ipython-input-7-cadea068683f>:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/60], Loss: 586.3003, Train Acc: 29.00%, Val Acc: 54.62%\n",
      "Epoch [52/60], Loss: 508.5014, Train Acc: 52.75%, Val Acc: 57.50%\n",
      "Epoch [53/60], Loss: 473.3962, Train Acc: 63.02%, Val Acc: 58.38%\n",
      "Epoch [54/60], Loss: 457.1307, Train Acc: 67.58%, Val Acc: 58.75%\n",
      "Epoch [55/60], Loss: 448.0179, Train Acc: 69.71%, Val Acc: 58.88%\n",
      "Model saved: affectnet_convnext_epoch55.pt\n",
      "Epoch [56/60], Loss: 442.8233, Train Acc: 71.13%, Val Acc: 58.50%\n",
      "Epoch [57/60], Loss: 438.4933, Train Acc: 72.11%, Val Acc: 56.62%\n",
      "Epoch [58/60], Loss: 435.3070, Train Acc: 73.06%, Val Acc: 58.38%\n",
      "Early stopping triggered. Best validation accuracy: 58.88%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optimized Data Augmentation (Less Aggressive)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomGrayscale(p=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "train_data_path = \"/content/affectnet/AffectNet/train\"\n",
    "val_data_path = \"/content/affectnet/AffectNet/val\"\n",
    "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_path, transform=transform)\n",
    "\n",
    "# Compute Class Weights\n",
    "class_counts = Counter(train_dataset.targets)\n",
    "num_samples = sum(class_counts.values())\n",
    "weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights[6] *= 1.5\n",
    "weights[7] *= 1.3\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Load Data (Batch Size Adjusted)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Load Model (ConvNeXt-Small)\n",
    "model = models.convnext_small(weights=models.ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Load Checkpoint\n",
    "checkpoint_path = \"/content/affectnet_convnext_epoch50.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Remove classifier keys from checkpoint (to prevent mismatches)\n",
    "for key in list(checkpoint.keys()):\n",
    "    if \"classifier\" in key:\n",
    "        del checkpoint[key]\n",
    "\n",
    "# Load checkpoint into model\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# Modify Classifier AFTER Loading Checkpoint\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.Dropout(0.5),  # ğŸ”¹ Increased Dropout\n",
    "    nn.Linear(model.classifier[2].in_features, 8)\n",
    ")\n",
    "\n",
    "print(\"Checkpoint successfully loaded!\")\n",
    "\n",
    "# Move Model to Device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Loss, Optimizer & Scheduler\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.3)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-6)\n",
    "\n",
    "# Dynamic Learning Rate (ReduceLROnPlateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Stochastic Weight Averaging (SWA)\n",
    "swa_model = optim.swa_utils.AveragedModel(model)\n",
    "swa_scheduler = optim.swa_utils.SWALR(optimizer, swa_lr=5e-7, anneal_strategy=\"cos\", anneal_epochs=5)\n",
    "\n",
    "# Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Fine-tune for 10 More Epochs (51-60)\n",
    "best_val_acc = 0.0\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"\\nFine-tuning with Stronger Regularization and Stability...\\n\")\n",
    "\n",
    "for epoch in range(51, 61):\n",
    "    model.train()\n",
    "    running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward Pass with Mixed Precision\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation with Mixed Precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # Fix: Move `scheduler.step(val_accuracy)` Below Validation Phase\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/60], Loss: {running_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save Model Every 5 Epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"affectnet_convnext_epoch{epoch}.pt\")\n",
    "        print(f\"Model saved: affectnet_convnext_epoch{epoch}.pt\")\n",
    "\n",
    "    #  Early Stopping\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tykype89pRqG",
    "outputId": "c538007b-37ea-4c4d-c780-940bdd5e52cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\n",
      "Fine-tuning with Better Regularization...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2319f1e22c99>:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "<ipython-input-8-2319f1e22c99>:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/65], Loss: 673.9786, Train Acc: 10.41%, Val Acc: 10.12%\n",
      "Epoch [57/65], Loss: 655.7998, Train Acc: 13.08%, Val Acc: 17.38%\n",
      "Epoch [58/65], Loss: 643.4017, Train Acc: 15.10%, Val Acc: 23.12%\n",
      "Epoch [59/65], Loss: 633.9894, Train Acc: 17.13%, Val Acc: 26.50%\n",
      "Epoch [60/65], Loss: 631.8518, Train Acc: 17.66%, Val Acc: 26.75%\n",
      "Model saved: affectnet_convnext_epoch60.pt\n",
      "Epoch [61/65], Loss: 622.4510, Train Acc: 19.74%, Val Acc: 34.50%\n",
      "Epoch [62/65], Loss: 608.9430, Train Acc: 23.02%, Val Acc: 39.50%\n",
      "Epoch [63/65], Loss: 596.3580, Train Acc: 26.29%, Val Acc: 42.38%\n",
      "Epoch [64/65], Loss: 589.6599, Train Acc: 28.05%, Val Acc: 45.38%\n",
      "Epoch [65/65], Loss: 581.9471, Train Acc: 30.44%, Val Acc: 47.00%\n",
      "Model saved: affectnet_convnext_epoch65.pt\n",
      "\n",
      "Training complete! Final model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optimized Data Augmentation (Prevent Overfitting)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),  # Avoid aggressive cropping\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomGrayscale(p=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "train_data_path = \"/content/affectnet/AffectNet/train\"\n",
    "val_data_path = \"/content/affectnet/AffectNet/val\"\n",
    "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_path, transform=transform)\n",
    "\n",
    "# Compute Class Weights\n",
    "class_counts = Counter(train_dataset.targets)\n",
    "num_samples = sum(class_counts.values())\n",
    "weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights[6] *= 1.5\n",
    "weights[7] *= 1.3\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Load Data (Batch Size Adjusted)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Load Model (ConvNeXt-Small)\n",
    "model = models.convnext_small(weights=models.ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Load Checkpoint\n",
    "checkpoint_path = \"/content/affectnet_convnext_epoch55.pt\"  # Resuming from last checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Remove classifier keys from checkpoint (to prevent mismatches)\n",
    "for key in list(checkpoint.keys()):\n",
    "    if \"classifier\" in key:\n",
    "        del checkpoint[key]\n",
    "\n",
    "# Load checkpoint into model\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# Modify Classifier AFTER Loading Checkpoint\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.Dropout(0.6),  # ğŸ”¹ Increased Dropout\n",
    "    nn.Linear(model.classifier[2].in_features, 8)\n",
    ")\n",
    "\n",
    "print(\"Checkpoint successfully loaded!\")\n",
    "\n",
    "# Move Model to Device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Loss, Optimizer & Scheduler\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.3)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-7, weight_decay=1e-5)  # ğŸ”¹ Lower LR, Higher weight decay\n",
    "\n",
    "# Learning Rate Scheduler (Cosine Annealing with Warm Restarts)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "\n",
    "# Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Fine-tune for 10 More Epochs (56-65)\n",
    "best_val_acc = 0.0\n",
    "early_stopping_patience = 5  # ğŸ”¹ Increased patience\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"\\nFine-tuning with Better Regularization...\\n\")\n",
    "\n",
    "for epoch in range(56, 66):\n",
    "    model.train()\n",
    "    running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward Pass with Mixed Precision\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation with Mixed Precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # Apply Scheduler Step After Validation\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch}/65], Loss: {running_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save Model Every 5 Epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"affectnet_convnext_epoch{epoch}.pt\")\n",
    "        print(f\"Model saved: affectnet_convnext_epoch{epoch}.pt\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "\n",
    "# Save Final Model\n",
    "torch.save(model.state_dict(), \"affectnet_convnext_final.pt\")\n",
    "print(\"\\nTraining complete! Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NvrPStY5Mhc",
    "outputId": "3aadcc39-01af-4b19-ab1a-9c374869ad09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\n",
      "Fine-tuning from Epoch 66-80...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-585b6f97c896>:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "<ipython-input-9-585b6f97c896>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/80], Loss: 345.5571, Train Acc: 67.06%, Val Acc: 58.88%\n",
      "Epoch [67/80], Loss: 294.6591, Train Acc: 75.61%, Val Acc: 56.50%\n",
      "Epoch [68/80], Loss: 282.5745, Train Acc: 77.54%, Val Acc: 57.12%\n",
      "Epoch [69/80], Loss: 274.6962, Train Acc: 78.81%, Val Acc: 58.50%\n",
      "Early stopping triggered. Best validation accuracy: 58.88%\n",
      "\n",
      "Training complete! Final model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "train_data_path = \"/content/affectnet/AffectNet/train\"\n",
    "val_data_path = \"/content/affectnet/AffectNet/val\"\n",
    "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_path, transform=transform)\n",
    "\n",
    "# Compute Class Weights\n",
    "class_counts = Counter(train_dataset.targets)\n",
    "num_samples = sum(class_counts.values())\n",
    "weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights[6] *= 1.5\n",
    "weights[7] *= 1.3\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Load Data\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Load Model (ConvNeXt-Small)\n",
    "model = models.convnext_small(weights=models.ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Load Checkpoint from Epoch 65\n",
    "checkpoint_path = \"/content/affectnet_convnext_epoch65.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "for key in list(checkpoint.keys()):\n",
    "    if \"classifier\" in key:\n",
    "        del checkpoint[key]\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# Modify Classifier AFTER Loading Checkpoint\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.Dropout(0.5),  # Increased dropout for better generalization\n",
    "    nn.Linear(model.classifier[2].in_features, 8)\n",
    ")\n",
    "\n",
    "print(\"Checkpoint successfully loaded!\")\n",
    "\n",
    "# Move Model to Device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Loss, Optimizer & Scheduler\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=5e-6)\n",
    "\n",
    "# OneCycleLR Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=15)\n",
    "\n",
    "# Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Fine-tune for 15 More Epochs (66-80)\n",
    "best_val_acc = 0.0\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"\\nFine-tuning from Epoch 66-80...\\n\")\n",
    "\n",
    "for epoch in range(66, 81):\n",
    "    model.train()\n",
    "    running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch [{epoch}/80], Loss: {running_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save Model Every 5 Epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"affectnet_convnext_epoch{epoch}.pt\")\n",
    "        print(f\"Model saved: affectnet_convnext_epoch{epoch}.pt\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "\n",
    "# Save Final Model\n",
    "torch.save(model.state_dict(), \"affectnet_convnext_final.pt\")\n",
    "print(\"\\nTraining complete! Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCBdENxVAJzr",
    "outputId": "da0e1c76-b4de-403b-c012-189bb88558ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint successfully loaded!\n",
      "\n",
      "Fine-tuning from Epoch 70-80...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-ad6af5f85984>:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "<ipython-input-11-ad6af5f85984>:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/80], Loss: 297.4104, Train Acc: 70.18%, Val Acc: 58.38%\n",
      "Model saved: affectnet_convnext_epoch70.pt\n",
      "Epoch [71/80], Loss: 238.0796, Train Acc: 77.74%, Val Acc: 58.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "\n",
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Augmentation (Slightly Stronger)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.75, 1.0)),  # Less cropping\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(6),  # Lower rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomGrayscale(p=0.05),  # Less grayscale conversion\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "train_data_path = \"/content/affectnet/AffectNet/train\"\n",
    "val_data_path = \"/content/affectnet/AffectNet/val\"\n",
    "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_path, transform=transform)\n",
    "\n",
    "# Compute Class Weights\n",
    "class_counts = Counter(train_dataset.targets)\n",
    "num_samples = sum(class_counts.values())\n",
    "weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Load Data\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Load Model (ConvNeXt-Small)\n",
    "model = models.convnext_small(weights=models.ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#  Load Checkpoint from Epoch 69\n",
    "checkpoint_path = \"/content/affectnet_convnext_epoch65.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "for key in list(checkpoint.keys()):\n",
    "    if \"classifier\" in key:\n",
    "        del checkpoint[key]\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "#  Modify Classifier AFTER Loading Checkpoint\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.Dropout(0.3),  # ğŸ”¹ Reduced dropout\n",
    "    nn.Linear(model.classifier[2].in_features, 8)\n",
    ")\n",
    "\n",
    "print(\"Checkpoint successfully loaded!\")\n",
    "\n",
    "#  Move Model to Device\n",
    "model = model.to(device)\n",
    "\n",
    "#  Define Loss, Optimizer & Scheduler\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.05)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-6)\n",
    "\n",
    "# OneCycleLR Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=10)\n",
    "\n",
    "#  Mixed Precision Training\n",
    "scaler = GradScaler()\n",
    "\n",
    "#  Fine-tune for 10 More Epochs (70-80)\n",
    "best_val_acc = 58.88  # Start from previous best\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"\\nFine-tuning from Epoch 70-80...\\n\")\n",
    "\n",
    "for epoch in range(70, 81):\n",
    "    model.train()\n",
    "    running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch [{epoch}/80], Loss: {running_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    #  Save Model Every 5 Epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"affectnet_convnext_epoch{epoch}.pt\")\n",
    "        print(f\"Model saved: affectnet_convnext_epoch{epoch}.pt\")\n",
    "\n",
    "    #  Early Stopping\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "\n",
    "#  Save Final Model\n",
    "torch.save(model.state_dict(), \"affectnet_convnext_final.pt\")\n",
    "print(\"\\nTraining complete! Final model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
